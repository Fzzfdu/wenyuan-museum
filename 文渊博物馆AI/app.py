import streamlit as st
from dotenv import load_dotenv
import os, faiss, numpy as np, asyncio
from sentence_transformers import SentenceTransformer
from openai import OpenAI, AsyncOpenAI
import streamlit as st
from PIL import Image
import base64, io
import speech_recognition as sr  
from gtts import gTTS               
import pygame                      
import tempfile
import streamlit.components.v1 as components

st.set_page_config(page_title="æ–‡æ¸Šåšç‰©é¦†æ™ºèƒ½å¯¼è§ˆ", page_icon="ğŸ–¼")
load_dotenv()
client = OpenAI(
    api_key=os.getenv("DASHSCOPE_API_KEY"),
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
)
aclient = AsyncOpenAI(
    api_key=os.getenv("DASHSCOPE_API_KEY"),
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
)
embedder = SentenceTransformer("BAAI/bge-small-zh-v1.5", device="cpu")

# è¯»å–å¹¶æ„å»ºå‘é‡åº“
@st.cache_resource
def load_data():
    with open("museum_data.txt", "r", encoding="utf-8") as f:
        raw = f.read().strip().split("\n\n")
        docs = [b.strip() for b in raw if b.strip()]
    
    if not os.path.exists("faiss.index"):
        embeddings = embedder.encode(docs, normalize_embeddings=True)
        dim = embeddings.shape[1]
        index = faiss.IndexFlatIP(dim)
        index.add(embeddings)
        faiss.write_index(index, "faiss.index")
        np.save("docs.npy", np.array(docs, dtype=object))
    else:
        index = faiss.read_index("faiss.index")
        docs = np.load("docs.npy", allow_pickle=True).tolist()
    
    return docs, index

docs, index = load_data()

# ç•Œé¢

st.title("ğŸ–¼ æ–‡æ¸Šåšç‰©é¦† Â· æ™ºèƒ½å¯¼è§ˆå‘˜")
# ===== æ‹ç…§è¯†æ–‡ç‰©ï¼ˆ2025.11.23 ä¸‡èƒ½ç‰ˆï¼Œæ°¸ä¸æŠ¥é”™ï¼‰=====
st.markdown("### ğŸ“¸ æ‹ä¸€å¼ æ–‡ç‰©ç…§ç‰‡ï¼Œæˆ‘æ¥å‘Šè¯‰ä½ å®ƒæ˜¯è°")
uploaded_file = st.file_uploader("ä¸Šä¼ æ–‡ç‰©ç…§ç‰‡ï¼ˆæ”¯æŒä»»ä½•æ ¼å¼ï¼‰", type=["jpg", "jpeg", "png", "webp", "bmp"])

if uploaded_file is not None:
    image = Image.open(uploaded_file).convert("RGB")   # â† å…³é”®ï¼å¼ºåˆ¶è½¬æˆ RGB
    st.image(image, caption="æ‚¨ä¸Šä¼ çš„æ–‡ç‰©", width=300)
    
    with st.spinner("æ­£åœ¨ç”¨é€šä¹‰åƒé—®å¤šæ¨¡æ€æ¨¡å‹è¯†åˆ«..."):
        buffered = io.BytesIO()
        image.save(buffered, format="JPEG", quality=95)  # ç°åœ¨ä¸€å®šèƒ½å­˜
        img_str = base64.b64encode(buffered.getvalue()).decode()
        
        response = client.chat.completions.create(
            model="qwen-vl-plus",
            messages=[{
                "role": "user",
                "content": [
                    {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{img_str}"}},
                    {"type": "text", "text": f"è¿™æ˜¯æ–‡æ¸Šåšç‰©é¦†çš„å“ªä»¶å±•å“ï¼Ÿè¯·ç»“åˆæˆ‘æä¾›çš„èµ„æ–™åˆ¤æ–­ï¼ˆåªå›ç­”æœ€åŒ¹é…çš„ä¸€ä»¶ï¼‰ï¼š\n" + "\n\n".join(docs)}
                ]
            }],
            temperature=0.3,
            max_tokens=500
        )
        result = response.choices[0].message.content
        st.success("è¯†åˆ«ç»“æœï¼š")
        st.markdown(result)
# ===== ç¬¬2å¤©ï¼šè¯­éŸ³è¾“å…¥ + å¥³å£°æ’­æŠ¥ =====
# ===== äº‘ç«¯è¯­éŸ³è¾“å…¥ï¼ˆWeb Speech APIï¼ŒStreamlit Cloud å®Œç¾æ”¯æŒï¼‰=====
st.markdown("### ğŸ¤ è¯­éŸ³é—®æˆ‘ï¼ˆæµè§ˆå™¨è‡ªåŠ¨è¯†åˆ«ï¼‰")

if st.button("ğŸ¤ ç‚¹æˆ‘è¯´è¯", key="web_voice"):
    st.write("è¯·å…è®¸æµè§ˆå™¨è®¿é—®éº¦å…‹é£...")
    
    # JavaScript ä»£ç ï¼ˆæµè§ˆå™¨å†…ç½®è¯­éŸ³è¯†åˆ«ï¼Œæ— éœ€ pyaudioï¼‰
    js_code = '''
    <script>
    if ('webkitSpeechRecognition' in window || 'SpeechRecognition' in window) {
        const recognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
        recognition.lang = 'zh-CN';
        recognition.interimResults = false;
        recognition.maxAlternatives = 1;
        
        recognition.onresult = function(event) {
            const transcript = event.results[0][0].transcript;
            parent.postMessage({type: 'streamlit:setComponentValue', value: transcript}, '*');
        };
        recognition.onerror = function(event) {
            parent.postMessage({type: 'streamlit:setComponentValue', value: 'è¯†åˆ«å¤±è´¥: ' + event.error}, '*');
        };
        recognition.start();
    } else {
        st.write("è¯·ç”¨ Chrome æˆ– Edge æµè§ˆå™¨");
    }
    </script>
    '''
    st.components.v1.html(js_code, height=0)
    
    # æ¥æ”¶ç»“æœï¼ˆç”¨ session_state ç›‘å¬ï¼‰
    if 'voice_result' not in st.session_state:
        st.session_state.voice_result = ''
    
    voice_text = st.text_input("è¯†åˆ«ç»“æœï¼ˆè‡ªåŠ¨å¡«å…¥ï¼‰", value=st.session_state.voice_result, key="voice_output")
    
    if voice_text and voice_text != 'è¯†åˆ«å¤±è´¥: ':
        st.success(f"æˆ‘å¬åˆ°ä½ è¯´ï¼š{voice_text}")
        
        # ç›´æ¥è§¦å‘å¤šæ™ºèƒ½ä½“å›ç­”ï¼ˆç”¨ voice_text æ›¿æ¢ promptï¼‰
        with st.chat_message("user"):
            st.markdown(voice_text)
        with st.chat_message("assistant"):
            with st.spinner("3ä½AIå¯¼æ¸¸æ­£åœ¨è®¨è®º..."):
                # ä½ çš„æ£€ç´¢ + å¤šæ™ºèƒ½ä½“ä»£ç ï¼ˆä¿æŒä¸å˜ï¼‰
                query_vec = embedder.encode([voice_text], normalize_embeddings=True)
                D, I = index.search(query_vec, k=3)
                context = "\n\n".join([f"ã€èµ„æ–™{i+1}ã€‘\n{docs[i]}" for i, idx in enumerate(I[0])])
                
                expert = client.chat.completions.create(
                    model="qwen-max",
                    messages=[{"role": "user", "content": f"èµ„æ–™ï¼š{context}\né—®é¢˜ï¼š{voice_text}\nè¯·ä¸“ä¸šè®²è§£ï¼š"}],
                    temperature=0.3
                ).choices[0].message.content
                
                story = client.chat.completions.create(
                    model="qwen-max",
                    messages=[{"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¼šè®²ç¡å‰æ•…äº‹çš„å¯¼æ¸¸"},
                              {"role": "user", "content": f"è®²æˆæ•…äº‹ï¼š{expert}"}],
                    temperature=0.7
                ).choices[0].message.content
                
                english = client.chat.completions.create(
                    model="qwen-max",
                    messages=[{"role": "user", "content": f"ç¿»è¯‘æˆè‹±æ–‡ï¼š{expert}"}],
                    temperature=0.3
                ).choices[0].message.content
                
                final_answer = f"**ä¸“ä¸šè®²è§£ï¼š**\n{expert}\n\n**æ•…äº‹ç‰ˆï¼š**\n{story}\n\n**Englishï¼š**\n{english}"
                st.markdown(final_answer)
                
                # å¥³å£°æ’­æŠ¥ï¼ˆä½ å·²æœ‰çš„ç»ˆæç‰ˆï¼‰
                play_tts_final(final_answer)
        
        st.session_state.voice_result = ''  # æ¸…ç©º
st.caption("å·²åŠ è½½å±•å“æ•°é‡ï¼š"+str(len(docs))+" ä»¶  â”‚  æ¨¡å‹ï¼šé€šä¹‰åƒé—® Qwen-Max")

if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "æ‚¨å¥½ï¼æˆ‘æ˜¯æ–‡æ¸Šåšç‰©é¦†AIå¯¼è§ˆå‘˜ï¼Œè¯·é—®æ‚¨æƒ³äº†è§£å“ªä»¶å±•å“ï¼Ÿ"}]

for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

if prompt := st.chat_input("é—®æˆ‘ä»»ä½•å…³äºå±•å“çš„é—®é¢˜ï½"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)
    
    with st.chat_message("assistant"):
        with st.spinner("3ä½AIå¯¼æ¸¸æ­£åœ¨é›†ä½“è®¨è®º..."):
            # 1. å…ˆæ£€ç´¢ï¼ˆä½ åŸæ¥å°±æœ‰çš„ä»£ç ï¼Œä¿ç•™ï¼‰
            query_vec = embedder.encode([prompt], normalize_embeddings=True)
            D, I = index.search(query_vec, k=3)
            context = "\n\n".join([f"ã€èµ„æ–™{i+1}ã€‘\n{docs[i]}" for i, idx in enumerate(I[0])])
            
            # 2. å¤šæ™ºèƒ½ä½“å¼€å§‹ï¼
            expert = client.chat.completions.create(
                model="qwen-max",
                messages=[{"role": "user", "content": f"èµ„æ–™ï¼š{context}\né—®é¢˜ï¼š{prompt}\nè¯·ç”¨ä¸“ä¸šè¯­æ°”è¯¦ç»†è®²è§£è¿™ä»¶æ–‡ç‰©ï¼š"}],
                temperature=0.3
            ).choices[0].message.content
            
            story = client.chat.completions.create(
                model="qwen-max",
                messages=[{"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¼šè®²ç¡å‰æ•…äº‹çš„å¯¼æ¸¸ï¼Œè¦ç”ŸåŠ¨æœ‰è¶£"},
                          {"role": "user", "content": f"æŠŠè¿™ä»¶æ–‡ç‰©è®²æˆä¸€ä¸ªå¸å¼•äººçš„ç¡å‰æ•…äº‹ï¼š{expert}"}],
                temperature=0.7
            ).choices[0].message.content
            
            english = client.chat.completions.create(
                model="qwen-max",
                messages=[{"role": "user", "content": f"æŠŠè¿™æ®µç¿»è¯‘æˆè‡ªç„¶æµåˆ©çš„è‹±æ–‡ï¼š{expert}"}],
                temperature=0.3
            ).choices[0].message.content
            
            final_answer = f"**ã€ä¸“ä¸šè®²è§£ã€‘**\n{expert}\n\n**ã€ç¡å‰æ•…äº‹ç‰ˆã€‘**\n{story}\n\n**ã€English Guideã€‘**\n{english}"
            st.markdown(final_answer)
            st.session_state.messages.append({"role": "assistant", "content": final_answer})
            import edge_tts
            import pygame
            import tempfile
            import time
            import os

            def play_tts_final(text):
                # å…ˆç”Ÿæˆæ–‡ä»¶
                communicate = edge_tts.Communicate(text, "zh-CN-XiaoxiaoNeural")
                tmp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".mp3")
                communicate.save_sync(tmp_file.name)
                tmp_path = tmp_file.name
                tmp_file.close()  # å…ˆå…³é—­å¥æŸ„

                # å»¶è¿Ÿ 0.3 ç§’ç¡®ä¿æ–‡ä»¶å®Œå…¨å†™å…¥
                time.sleep(0.3)

                # æ’­æ”¾
                pygame.mixer.init()
                pygame.mixer.music.load(tmp_path)
                pygame.mixer.music.play()
                
                # é˜»å¡ç­‰å¾…æ’­å®Œ
                while pygame.mixer.music.get_busy():
                    time.sleep(0.1)
                
                # æ’­å®Œå†åˆ æ–‡ä»¶
                try:
                    os.unlink(tmp_path)
                except:
                    pass  # åˆ ä¸æ‰ä¹Ÿæ²¡äº‹

            st.write("æ­£åœ¨ç”¨å¾®è½¯æ™“æ™“å¥³å£°æ’­æŠ¥...")
            play_tts_final(final_answer)
